{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef36f535-4bdc-4e2b-a22a-179372324b26",
   "metadata": {},
   "source": [
    "![walmartecomm](walmartecomm.jpg)\n",
    "\n",
    "Here is the short backgroun about the project: Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n",
    "\n",
    "In this project, you have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. You will be working with two data sources: grocery sales and complementary data.\n",
    "\n",
    "# `grocery_sales`\n",
    "- `\"index\"` - unique ID of the row\n",
    "- `\"Store_ID\"` - the store number\n",
    "- `\"Date\"` - the week of sales\n",
    "- `\"Weekly_Sales\"` - sales for the given store\n",
    "\n",
    "Also, you have the `extra_data.parquet` file that contains complementary data:\n",
    "\n",
    "# `extra_data.parquet`\n",
    "- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n",
    "- `\"Temperature\"` - Temperature on the day of sale\n",
    "- `\"Fuel_Price\"` - Cost of fuel in the region\n",
    "- `\"CPI\"` â€“ Prevailing consumer price index\n",
    "- `\"Unemployment\"` - The prevailing unemployment rate\n",
    "- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n",
    "- `\"Dept\"` - Department Number in each store\n",
    "- `\"Size\"` - size of the store\n",
    "- `\"Type\"` - type of the store (depends on `Size` column)\n",
    "\n",
    "You will need to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n",
    "- `\"Store_ID\"`\n",
    "- `\"Month\"`\n",
    "- `\"Dept\"`\n",
    "- `\"IsHoliday\"`\n",
    "- `\"Weekly_Sales\"`\n",
    "- `\"CPI\"`\n",
    "- \"`\"Unemployment\"`\"\n",
    "\n",
    "After merging and cleaning the data, you will have to analyze monthly sales of Walmart and store the results of your analysis as the `agg_data` variable that should look like:\n",
    "\n",
    "|  Month | Weekly_Sales  | \n",
    "|---|---|\n",
    "| 1.0  |  33174.178494 |\n",
    "|  2.0 |  34333.326579 |\n",
    "|  ... | ...  |  \n",
    "\n",
    "Finally, you should save the `clean_data` and `agg_data` as the csv files.\n",
    "\n",
    "It is recommended to use `pandas` for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d64ff1-a4ca-4a82-a8b4-e210244dedc1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 94,
    "lastExecutedAt": 1729333655683,
    "lastExecutedByKernel": "9138f55a-30f7-469d-81f8-ce7279ea2349",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nimport os \n\n# Extract function to read the parquet data and then merge with the grocery sales data \ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n\n# Calling the extract() function and storing it as the \"merged_df\" variable\nmerged_df = extract(grocery_sales, \"extra_data.parquet\")",
    "outputsMetadata": {
     "0": {
      "height": 122,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "grocery_sales=pd.read_csv('grocery_sales.csv')\n",
    "\n",
    "# Extract function to read the parquet data and then merge with the grocery sales data \n",
    "def extract(store_data, extra_data):\n",
    "    extra_df = pd.read_parquet(extra_data)\n",
    "    merged_df = store_data.merge(extra_df, on = \"index\")\n",
    "    return merged_df\n",
    "\n",
    "# Calling the extract() function and storing it as the \"merged_df\" variable\n",
    "merged_df = extract(grocery_sales, \"extra_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d3f9fc64-0bd4-4a6f-bbc0-51d30051f98e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 61,
    "lastExecutedAt": 1729330867171,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Checking the output of the merged file\nprint(merged_df.head())",
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  Store_ID       Date  Dept  ...         CPI  Unemployment  Type      Size\n",
      "0      0         1 2010-02-05     1  ...  211.096358         8.106   3.0  151315.0\n",
      "1      1         1 2010-02-05    26  ...  211.096358         8.106   3.0  151315.0\n",
      "2      2         1 2010-02-05    17  ...  211.096358         8.106   3.0  151315.0\n",
      "3      3         1 2010-02-05    45  ...  211.096358           NaN   3.0  151315.0\n",
      "4      4         1 2010-02-05    28  ...  211.096358           NaN   3.0  151315.0\n",
      "\n",
      "[5 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "# Checking the output of the merged file\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "958b7701-366e-4544-8e1b-33e21ab17df7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1729330867223,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Checking which columns have missing values\nmerged_df.isna().sum()",
    "outputsMetadata": {
     "0": {
      "height": 543,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "0": [
          0,
          0,
          39,
          0,
          38,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          47,
          37,
          1,
          1
         ],
         "index": [
          "index",
          "Store_ID",
          "Date",
          "Dept",
          "Weekly_Sales",
          "IsHoliday",
          "Temperature",
          "Fuel_Price",
          "MarkDown1",
          "MarkDown2",
          "MarkDown3",
          "MarkDown4",
          "MarkDown5",
          "CPI",
          "Unemployment",
          "Type",
          "Size"
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "string"
          },
          {
           "name": "0",
           "type": "integer"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 17,
       "truncation_type": null
      },
      "text/plain": [
       "index            0\n",
       "Store_ID         0\n",
       "Date            39\n",
       "Dept             0\n",
       "Weekly_Sales    38\n",
       "IsHoliday        0\n",
       "Temperature      0\n",
       "Fuel_Price       0\n",
       "MarkDown1        0\n",
       "MarkDown2        0\n",
       "MarkDown3        0\n",
       "MarkDown4        1\n",
       "MarkDown5        1\n",
       "CPI             47\n",
       "Unemployment    37\n",
       "Type             1\n",
       "Size             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking which columns have missing values\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6d3c25e2-e7d8-4c33-9be0-d45f03b2cf43",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 612,
    "lastExecutedAt": 1729330867835,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Creating the transform() function with one parameter: \"raw_data\"\n# This function takes the merged file as an input, fills missing numerical values, adds a column \"Month\",  then filters the rows with weekly sales over $10000 and drops the unncessary columns\ndef transform(raw_data):\n  merged_df['Date']=merged_df['Date'].fillna(method='ffill')\n  merged_df['Temperature']=merged_df['Temperature'].fillna(method='ffill')\n  merged_df['CPI']=merged_df['CPI'].fillna(method='ffill')\n  merged_df['Unemployment']=merged_df['Unemployment'].fillna(method='ffill')\n  merged_df['Unemployment']=merged_df['Unemployment'].fillna(method='ffill')\n  merged_df['MarkDown4']=merged_df['MarkDown4'].fillna(method='ffill')\n  merged_df['MarkDown5']=merged_df['MarkDown5'].fillna(method='ffill')  \n  merged_df['Size']=merged_df['Size'].fillna(method='ffill')\n  merged_df['Type']=merged_df['Type'].fillna(method='ffill')\n  merged_df['Weekly_Sales']=merged_df['Weekly_Sales'].fillna(merged_df.groupby(['Store_ID', 'Dept','Date'])['Weekly_Sales'].transform('mean'))  \n  merged_df['Month']=merged_df['Date'].dt.month\n  merged_df2=merged_df[merged_df['Weekly_Sales']>10000]\n  merged_df3=merged_df2[['Store_ID', 'Month', 'Dept', 'IsHoliday', 'Weekly_Sales', 'CPI', 'Unemployment']]\n  return merged_df3\n  \n  \npass"
   },
   "outputs": [],
   "source": [
    "# Creating the transform() function with one parameter: \"raw_data\"\n",
    "# This function takes the merged file as an input, fills missing numerical values, adds a column \"Month\",  then filters the rows with weekly sales over $10000 and drops the unncessary columns\n",
    "def transform(raw_data):\n",
    "  merged_df['Date']=merged_df['Date'].fillna(method='ffill')\n",
    "  merged_df['Temperature']=merged_df['Temperature'].fillna(method='ffill')\n",
    "  merged_df['CPI']=merged_df['CPI'].fillna(method='ffill')\n",
    "  merged_df['Unemployment']=merged_df['Unemployment'].fillna(method='ffill')\n",
    "  merged_df['Unemployment']=merged_df['Unemployment'].fillna(method='ffill')\n",
    "  merged_df['MarkDown4']=merged_df['MarkDown4'].fillna(method='ffill')\n",
    "  merged_df['MarkDown5']=merged_df['MarkDown5'].fillna(method='ffill')  \n",
    "  merged_df['Size']=merged_df['Size'].fillna(method='ffill')\n",
    "  merged_df['Type']=merged_df['Type'].fillna(method='ffill')\n",
    "  merged_df['Weekly_Sales']=merged_df['Weekly_Sales'].fillna(merged_df.groupby(['Store_ID', 'Dept','Date'])['Weekly_Sales'].transform('mean'))  \n",
    "  merged_df['Month']=merged_df['Date'].dt.month\n",
    "  merged_df2=merged_df[merged_df['Weekly_Sales']>10000]\n",
    "  merged_df3=merged_df2[['Store_ID', 'Month', 'Dept', 'IsHoliday', 'Weekly_Sales', 'CPI', 'Unemployment']]\n",
    "  return merged_df3\n",
    "  \n",
    "  \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "620b7289-06cd-4205-be9e-a50dc8d36cf0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 79,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1729330867914,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Calling  the transform() function and passing the merged DataFrame\nclean_data = transform(merged_df)"
   },
   "outputs": [],
   "source": [
    "# Calling  the transform() function and passing the merged DataFrame\n",
    "clean_data = transform(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6edff649-291c-4f27-ae55-8db3661c25b9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 61,
    "lastExecutedAt": 1729330867975,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Checking the clean data\nprint(clean_data.head())",
    "outputsMetadata": {
     "0": {
      "height": 143,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store_ID  Month  Dept  IsHoliday  Weekly_Sales         CPI  Unemployment\n",
      "0         1      2     1          0      24924.50  211.096358         8.106\n",
      "1         1      2    26          0      11737.12  211.096358         8.106\n",
      "2         1      2    17          0      13223.76  211.096358         8.106\n",
      "5         1      2    79          0      46729.77  211.096358         8.106\n",
      "6         1      2    55          0      21249.31  211.096358         8.106\n"
     ]
    }
   ],
   "source": [
    "# Checking the clean data\n",
    "print(clean_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b19b15e3-6624-47a9-927f-d3f12fe8212d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1729330868023,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Creating the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n    cleaned_data1=clean_data.groupby('Month')\n    result=cleaned_data1.agg(Avg_Sales=('Weekly_Sales','mean'))\n    result1=result.reset_index()\n    rounded=round(result1, 2)\n    agg_data=rounded\n    return agg_data\n    pass"
   },
   "outputs": [],
   "source": [
    "# Creating the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    cleaned_data1=clean_data.groupby('Month')\n",
    "    result=cleaned_data1.agg(Avg_Sales=('Weekly_Sales','mean'))\n",
    "    result1=result.reset_index()\n",
    "    rounded=round(result1, 2)\n",
    "    agg_data=rounded\n",
    "    return agg_data\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fe875e27-b0cf-4e52-994e-4ae1fe6e8876",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1729330868075,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Calling the avg_weekly_sales_per_month() function and passing the cleaned DataFrame\nagg_data=avg_weekly_sales_per_month(clean_data)"
   },
   "outputs": [],
   "source": [
    "# Calling the avg_weekly_sales_per_month() function and passing the cleaned DataFrame\n",
    "agg_data=avg_weekly_sales_per_month(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c35a041-10eb-4621-a51b-f6be80e62023",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1729330868127,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# CHecking the reulst of the avg_weekly_sales_per_month() function\nprint(agg_data)",
    "outputsMetadata": {
     "0": {
      "height": 290,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Month  Avg_Sales\n",
      "0       1   33174.18\n",
      "1       2   34340.44\n",
      "2       3   33227.31\n",
      "3       4   33413.25\n",
      "4       5   33339.89\n",
      "5       6   34582.47\n",
      "6       7   33930.77\n",
      "7       8   33644.79\n",
      "8       9   33266.59\n",
      "9      10   32731.06\n",
      "10     11   36594.03\n",
      "11     12   39248.98\n"
     ]
    }
   ],
   "source": [
    "# CHecking the reulst of the avg_weekly_sales_per_month() function\n",
    "print(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "921cb123-3153-4334-bdeb-9bb227fdc530",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1729330868179,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Creating the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n    saved_full_data=full_data.to_csv(full_data_file_path, index=False)\n    saved_agg_data=agg_data.to_csv(agg_data_file_path, index=False)\n    \n    pass"
   },
   "outputs": [],
   "source": [
    "# Creating the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "    saved_full_data=full_data.to_csv(full_data_file_path, index=False)\n",
    "    saved_agg_data=agg_data.to_csv(agg_data_file_path, index=False)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f518ad5c-214e-474b-80bd-827b0c0e1536",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 365,
    "lastExecutedAt": 1729330868544,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Calling the load() function and passing the cleaned and aggregated DataFrames with their paths\nload(clean_data, 'clean_data.csv',agg_data, 'agg_data.csv')"
   },
   "outputs": [],
   "source": [
    "# Calling the load() function and passing the cleaned and aggregated DataFrames with their paths\n",
    "load(clean_data, 'clean_data.csv',agg_data, 'agg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "61b5f58a-70cb-40b3-bdbe-20b4079276e3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1729330868591,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Now, we need to check whether the two csv files from the load() function exist in the current working directory \n# Creating the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\ndef validation(file_path):\n    file_exists=os.path.exists(file_path)\n    print(file_exists)\n    pass"
   },
   "outputs": [],
   "source": [
    "# Now, we need to check whether the two csv files from the load() function exist in the current working directory \n",
    "# Creating the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "def validation(file_path):\n",
    "    file_exists=os.path.exists(file_path)\n",
    "    print(file_exists)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df1659ff-41c4-4a92-9812-80c6eaa02b90",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1729330868639,
    "lastExecutedByKernel": "9a10e684-64f5-4fc3-9b0b-6cda15ff5b27",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Calling the validation() function and passing first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation('clean_data.csv')\nvalidation('agg_data.csv')",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Calling the validation() function and passing first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation('clean_data.csv')\n",
    "validation('agg_data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
